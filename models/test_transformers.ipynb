{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST MODELS INITIALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from '/home/cagnetta/shakespeare/models/../models/__init__.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import models\n",
    "import importlib\n",
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD A SIMPLE TEXT DATASET TO TEST SMALL LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "kroziwwhcimuzelmadqkacdlabuyoagzvbzagwnoftoblehbayjacqaatvqlbcoa\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "for i in range(97, 123):\n",
    "    vocab.append(chr(i))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)\n",
    "text = random.choices(vocab, weights=None, k=64)\n",
    "text = ''.join([c for c in text])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLevelTokenizer:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.vocab = []\n",
    "        for i in range(97, 123):\n",
    "            self.vocab.append(chr(i))\n",
    "            self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.i_to_s = { i:ch for i,ch in enumerate(self.vocab)}\n",
    "        self.s_to_i = { ch:i for i,ch in self.i_to_s.items()}\n",
    "\n",
    "    def encode(self,s):\n",
    "        return torch.tensor([self.s_to_i[c] for c in s]).long()\n",
    "\n",
    "    def decode(self,s):\n",
    "        return ''.join([self.i_to_s[i.item()] for i in s])\n",
    "\n",
    "tokenizer = CharacterLevelTokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class config:\n",
    "\n",
    "    block_size = 64 # context length\n",
    "    vocab_size = vocab_size\n",
    "\n",
    "    num_heads = 4\n",
    "    head_size = 32\n",
    "    embedding_dim = num_heads*head_size\n",
    "    num_layers = 3\n",
    "\n",
    "    dropout = 0.1\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. parameters: 608410, generated (64 tokens):\n",
      "aevbtbvmosiyjlahyvijjphcfquxkqcslfwaxedianvzyizyfqkmsrrppphpmusru\n"
     ]
    }
   ],
   "source": [
    "model = models.SLM(\n",
    "    vocab_size=config.vocab_size,\n",
    "    block_size=config.block_size,\n",
    "    embedding_dim=config.embedding_dim,\n",
    "    num_heads=config.num_heads,\n",
    "    num_layers=config.num_layers,\n",
    "    dropout=config.dropout\n",
    ")\n",
    "model = model.to(device=config.device)\n",
    "param_count = sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=config.device)\n",
    "generated = model.generate(context, num_tokens=64)\n",
    "print(f'Num. parameters: {param_count}, generated ({64} tokens):\\n{tokenizer.decode(generated[0])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rhm",
   "language": "python",
   "name": "rhm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
